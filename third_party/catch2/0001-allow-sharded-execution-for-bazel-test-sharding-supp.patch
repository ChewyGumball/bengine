From 2619e5decc7ba6380bba32aa0c305bd63b3ba891 Mon Sep 17 00:00:00 2001
From: Ben Dunkin <chewygumball@gmail.com>
Date: Sun, 11 Jul 2021 12:46:05 -0700
Subject: [PATCH] allow sharded execution for bazel test sharding support

refactor sharding so that it works with listing tests, not just executing them

make the integration test comprehensive

add documentation

fix build errors for windows

don't use f strings?

update test file comment
---
 docs/command-line.md                          |  13 ++
 src/catch2/catch_all.hpp                      |   1 +
 src/catch2/catch_config.cpp                   |   2 +
 src/catch2/catch_config.hpp                   |   5 +
 src/catch2/catch_session.cpp                  |   3 +
 .../interfaces/catch_interfaces_config.hpp    |   2 +
 src/catch2/internal/catch_commandline.cpp     |   6 +
 src/catch2/internal/catch_sharding.hpp        |  49 ++++++
 .../catch_test_case_registry_impl.cpp         |   3 +-
 tests/CMakeLists.txt                          |   3 +
 tests/TestScripts/testSharding.py             | 143 ++++++++++++++++++
 11 files changed, 229 insertions(+), 1 deletion(-)
 create mode 100644 src/catch2/internal/catch_sharding.hpp
 create mode 100644 tests/TestScripts/testSharding.py

diff --git a/docs/command-line.md b/docs/command-line.md
index 14511db9..ff0178b8 100644
--- a/docs/command-line.md
+++ b/docs/command-line.md
@@ -29,6 +29,7 @@
 [Specify the section to run](#specify-the-section-to-run)<br>
 [Filenames as tags](#filenames-as-tags)<br>
 [Override output colouring](#override-output-colouring)<br>
+[Test Sharding](#test-sharding)<br>
 
 Catch works quite nicely without any command line options at all - but for those times when you want greater control the following options are available.
 Click one of the following links to take you straight to that option - or scroll on to browse the available options.
@@ -67,6 +68,8 @@ Click one of the following links to take you straight to that option - or scroll
 <a href="#benchmark-no-analysis">                       `    --benchmark-no-analysis`</a><br />
 <a href="#benchmark-warmup-time">                       `    --benchmark-warmup-time`</a><br />
 <a href="#use-colour">                                  `    --use-colour`</a><br />
+<a href="#test-sharding">                               `    --shard-count`</a><br />
+<a href="#test-sharding">                               `    --shard-index`</a><br />
 
 </br>
 
@@ -418,6 +421,16 @@ processing of output.
 `--use-colour yes` forces coloured output, `--use-colour no` disables coloured
 output. The default behaviour is `--use-colour auto`.
 
+<a id="test-sharding"></a>
+## Test Sharding
+<pre>--shard-count <#number of shards>, --shard-index <#shard index to run></pre>
+When `--shard-count <#number of shards>` is used, the tests to execute will be split evenly in to the given number of sets,
+identified by indicies starting at 0. The tests in the set given by `--shard-index <#shard index to run>` will be executed.
+The default shard count is `1`, and the default index to run is `0`. If the shard index is greater than the number of shards,
+the last shard will be run.
+
+This is useful when you want to split test execution across multiple processes, as is done with [Bazel test sharding](https://docs.bazel.build/versions/main/test-encyclopedia.html#test-sharding). 
+
 ---
 
 [Home](Readme.md#top)
diff --git a/src/catch2/catch_all.hpp b/src/catch2/catch_all.hpp
index c6ab59ec..18a68496 100644
--- a/src/catch2/catch_all.hpp
+++ b/src/catch2/catch_all.hpp
@@ -82,6 +82,7 @@
 #include <catch2/internal/catch_result_type.hpp>
 #include <catch2/internal/catch_run_context.hpp>
 #include <catch2/internal/catch_section.hpp>
+#include <catch2/internal/catch_sharding.hpp>
 #include <catch2/internal/catch_singletons.hpp>
 #include <catch2/internal/catch_source_line_info.hpp>
 #include <catch2/internal/catch_startup_exception_registry.hpp>
diff --git a/src/catch2/catch_config.cpp b/src/catch2/catch_config.cpp
index 7a4a0bda..b8bff0ad 100644
--- a/src/catch2/catch_config.cpp
+++ b/src/catch2/catch_config.cpp
@@ -74,6 +74,8 @@ namespace Catch {
     double Config::minDuration() const                 { return m_data.minDuration; }
     TestRunOrder Config::runOrder() const              { return m_data.runOrder; }
     unsigned int Config::rngSeed() const               { return m_data.rngSeed; }
+    unsigned int Config::shardCount() const            { return m_data.shardCount; }
+    unsigned int Config::shardIndex() const            { return m_data.shardIndex; }
     UseColour Config::useColour() const                { return m_data.useColour; }
     bool Config::shouldDebugBreak() const              { return m_data.shouldDebugBreak; }
     int Config::abortAfter() const                     { return m_data.abortAfter; }
diff --git a/src/catch2/catch_config.hpp b/src/catch2/catch_config.hpp
index 10d0487a..1111f6f0 100644
--- a/src/catch2/catch_config.hpp
+++ b/src/catch2/catch_config.hpp
@@ -35,6 +35,9 @@ namespace Catch {
         int abortAfter = -1;
         unsigned int rngSeed = 0;
 
+        unsigned int shardCount = 1;
+        unsigned int shardIndex = 0;
+
         bool benchmarkNoAnalysis = false;
         unsigned int benchmarkSamples = 100;
         double benchmarkConfidenceInterval = 0.95;
@@ -98,6 +101,8 @@ namespace Catch {
         double minDuration() const override;
         TestRunOrder runOrder() const override;
         unsigned int rngSeed() const override;
+        unsigned int shardCount() const override;
+        unsigned int shardIndex() const override;
         UseColour useColour() const override;
         bool shouldDebugBreak() const override;
         int abortAfter() const override;
diff --git a/src/catch2/catch_session.cpp b/src/catch2/catch_session.cpp
index edfef717..2eba9e2a 100644
--- a/src/catch2/catch_session.cpp
+++ b/src/catch2/catch_session.cpp
@@ -16,6 +16,7 @@
 #include <catch2/catch_version.hpp>
 #include <catch2/interfaces/catch_interfaces_reporter.hpp>
 #include <catch2/internal/catch_startup_exception_registry.hpp>
+#include <catch2/internal/catch_sharding.hpp>
 #include <catch2/internal/catch_textflow.hpp>
 #include <catch2/internal/catch_windows_h_proxy.hpp>
 #include <catch2/reporters/catch_reporter_listening.hpp>
@@ -77,6 +78,8 @@ namespace Catch {
                     for (auto const& match : m_matches)
                         m_tests.insert(match.tests.begin(), match.tests.end());
                 }
+
+                m_tests = createShard(m_tests, *m_config);
             }
 
             Totals execute() {
diff --git a/src/catch2/interfaces/catch_interfaces_config.hpp b/src/catch2/interfaces/catch_interfaces_config.hpp
index f09b0c0f..be175865 100644
--- a/src/catch2/interfaces/catch_interfaces_config.hpp
+++ b/src/catch2/interfaces/catch_interfaces_config.hpp
@@ -73,6 +73,8 @@ namespace Catch {
         virtual std::vector<std::string> const& getTestsOrTags() const = 0;
         virtual TestRunOrder runOrder() const = 0;
         virtual unsigned int rngSeed() const = 0;
+        virtual unsigned int shardCount() const = 0;
+        virtual unsigned int shardIndex() const = 0;
         virtual UseColour useColour() const = 0;
         virtual std::vector<std::string> const& getSectionsToRun() const = 0;
         virtual Verbosity verbosity() const = 0;
diff --git a/src/catch2/internal/catch_commandline.cpp b/src/catch2/internal/catch_commandline.cpp
index 0cb160e0..1c0d6a3a 100644
--- a/src/catch2/internal/catch_commandline.cpp
+++ b/src/catch2/internal/catch_commandline.cpp
@@ -218,6 +218,12 @@ namespace Catch {
             | Opt( config.benchmarkWarmupTime, "benchmarkWarmupTime" )
                 ["--benchmark-warmup-time"]
                 ( "amount of time in milliseconds spent on warming up each test (default: 100)" )
+            | Opt( config.shardCount, "shard count" )
+                ["--shard-count"]
+                ( "split the tests to execute into this many groups" )
+            | Opt( config.shardIndex, "shard index" )
+                ["--shard-index"]
+                ( "index of the group of tests to execute (see --shard-count)" )
             | Arg( config.testsOrTags, "test name|pattern|tags" )
                 ( "which test or tests to use" );
 
diff --git a/src/catch2/internal/catch_sharding.hpp b/src/catch2/internal/catch_sharding.hpp
new file mode 100644
index 00000000..f90d8cc6
--- /dev/null
+++ b/src/catch2/internal/catch_sharding.hpp
@@ -0,0 +1,49 @@
+
+//              Copyright Catch2 Authors
+// Distributed under the Boost Software License, Version 1.0.
+//   (See accompanying file LICENSE_1_0.txt or copy at
+//        https://www.boost.org/LICENSE_1_0.txt)
+
+// SPDX-License-Identifier: BSL-1.0
+#ifndef CATCH_SHARDING_HPP_INCLUDED
+#define CATCH_SHARDING_HPP_INCLUDED
+
+#include <catch2/catch_session.hpp>
+
+#include <cmath>
+
+namespace Catch {
+
+    template<typename CONTAINER>
+    CONTAINER createShard(CONTAINER const& container, IConfig const& config) {
+        if (config.shardCount() > 1) {
+            size_t totalTestCount = container.size();
+
+            size_t shardCount = (std::min)(size_t(config.shardCount()), totalTestCount);
+            size_t shardIndex = (std::min)(size_t(config.shardIndex()), shardCount - 1);
+
+            double shardSize = totalTestCount / double(shardCount);
+
+            size_t startIndex = size_t(std::floor(shardIndex * shardSize));
+            size_t endIndex = size_t(std::floor((shardIndex + 1) * shardSize));
+
+            auto startIterator = std::next(container.begin(), startIndex);
+            auto endIterator = std::next(container.begin(), endIndex);
+
+            // Since we are calculating the end index with floating point numbers, but flooring 
+            // the value, we can't guarantee that the end index of the last shard lines up exactly
+            // with the end of input container. If we want the last shard, force the end index to
+            // be the end of the input container.
+            if (shardIndex == shardCount - 1) {
+                endIterator = container.end();
+            }
+
+            return CONTAINER(startIterator, endIterator);
+        } else {
+            return container;
+        }
+    }
+
+}
+
+#endif // CATCH_SHARDING_HPP_INCLUDED
diff --git a/src/catch2/internal/catch_test_case_registry_impl.cpp b/src/catch2/internal/catch_test_case_registry_impl.cpp
index 6ce7a1ad..dad78396 100644
--- a/src/catch2/internal/catch_test_case_registry_impl.cpp
+++ b/src/catch2/internal/catch_test_case_registry_impl.cpp
@@ -12,6 +12,7 @@
 #include <catch2/interfaces/catch_interfaces_registry_hub.hpp>
 #include <catch2/internal/catch_random_number_generator.hpp>
 #include <catch2/internal/catch_run_context.hpp>
+#include <catch2/internal/catch_sharding.hpp>
 #include <catch2/catch_test_case_info.hpp>
 #include <catch2/catch_test_spec.hpp>
 
@@ -110,7 +111,7 @@ namespace {
                 filtered.push_back(testCase);
             }
         }
-        return filtered;
+        return createShard(filtered, config);
     }
     std::vector<TestCaseHandle> const& getAllTestCasesSorted( IConfig const& config ) {
         return getRegistryHub().getTestCaseRegistry().getAllTestsSorted( config );
diff --git a/tests/CMakeLists.txt b/tests/CMakeLists.txt
index 1a490398..ce90ab4c 100644
--- a/tests/CMakeLists.txt
+++ b/tests/CMakeLists.txt
@@ -309,6 +309,9 @@ set_tests_properties(TagAlias PROPERTIES
 add_test(NAME RandomTestOrdering COMMAND ${PYTHON_EXECUTABLE}
   ${CATCH_DIR}/tests/TestScripts/testRandomOrder.py $<TARGET_FILE:SelfTest>)
 
+add_test(NAME TestSharding COMMAND ${PYTHON_EXECUTABLE}
+  ${CATCH_DIR}/tests/TestScripts/testSharding.py $<TARGET_FILE:SelfTest>)
+
 add_test(NAME CheckConvenienceHeaders
   COMMAND
     ${PYTHON_EXECUTABLE} ${CATCH_DIR}/tools/scripts/checkConvenienceHeaders.py
diff --git a/tests/TestScripts/testSharding.py b/tests/TestScripts/testSharding.py
new file mode 100644
index 00000000..fbb2697f
--- /dev/null
+++ b/tests/TestScripts/testSharding.py
@@ -0,0 +1,143 @@
+#!/usr/bin/env python3
+
+"""
+This test script verifies that sharding tests does change which tests are run.
+This is done by running the binary multiple times, once to list all the tests,
+once per shard to list the tests for that shard, and once again per shard to
+execute the tests. The sharded lists are compared to the full list to ensure
+none are skipped, duplicated, and that the order remains the same. This process
+is repeated for multiple command line argument combinations to ensure sharding
+works with different filters and test orderings.
+"""
+
+import itertools
+import multiprocessing
+import random
+import subprocess
+import sys
+import xml.etree.ElementTree as ET
+
+from collections import namedtuple
+
+TestCase = namedtuple("TestCase", ("shard_count", "order", "tags", "rng_seed"))
+
+def list_tests(self_test_exe, test_case, shard_index=None):
+    cmd = [
+        self_test_exe,
+        '--reporter', 'xml',
+        '--list-tests',
+        '--order', test_case.order,
+        '--rng-seed', str(test_case.rng_seed)
+    ]
+
+    if shard_index is not None:
+        cmd.extend([
+            "--shard-count", str(test_case.shard_count),
+            "--shard-index", str(shard_index)
+        ])
+
+    tags_arg = ','.join('[{}]~[.]'.format(t) for t in test_case.tags)
+    if tags_arg:
+        cmd.append(tags_arg)
+    process = subprocess.Popen(
+            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
+    stdout, stderr = process.communicate()
+    if stderr:
+        raise RuntimeError("Unexpected error output:\n" + process.stderr)
+
+    root = ET.fromstring(stdout)
+    result = [elem.text for elem in root.findall('./TestCase/Name')]
+
+    if len(result) < 2:
+        raise RuntimeError("Unexpectedly few tests listed (got {})".format(
+            len(result)))
+    return result
+
+def execute_tests(self_test_exe, test_case, shard_index):
+    cmd = [
+        self_test_exe,
+        '--reporter', 'xml',
+        '--order', test_case.order,
+        '--rng-seed', str(test_case.rng_seed),
+        "--shard-count", str(test_case.shard_count),
+        "--shard-index", str(shard_index)
+    ]
+
+    tags_arg = ','.join('[{}]~[.]'.format(t) for t in test_case.tags)
+    if tags_arg:
+        cmd.append(tags_arg)
+    process = subprocess.Popen(
+            cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
+    stdout, stderr = process.communicate()
+    if stderr:
+        raise RuntimeError("Unexpected error output:\n" + process.stderr)
+
+    root = ET.fromstring(stdout)
+    result = [elem.attrib["name"] for elem in root.findall('./Group/TestCase')]
+
+    if len(result) < 2:
+        raise RuntimeError("Unexpectedly few tests listed (got {})".format(
+            len(result)))
+    return result
+
+def check_listed_tests_match(all_tests, sharded_tests):
+    # Make sure the number of tests in the full list, and the sharded lists are the same.
+    total_test_count = len(all_tests)
+    sharded_test_count = sum([len(shard) for shard in sharded_tests])
+
+    assert total_test_count == sharded_test_count, (
+        "Sharded test count ({}) does not match the total test count ({})".format(sharded_test_count, total_test_count)
+    )
+
+    # Make sure all the tests in the shards are from the full list, in the same order. Together with the previous check, this
+    # ensures that all tests in the full list, are in the shards without duplication.
+    test_index = 0
+    for shard_index, shard in enumerate(sharded_tests):
+        for shard_test_index, test_name in enumerate(shard):
+            assert test_name == all_tests[test_index], (
+                "Sharding does not split the test list while maintaining order {}:\n'{}' vs '{}'".format(test_index, test_name, all_tests[test_index])
+            )
+
+            test_index += 1
+
+def check_listed_and_executed_tests_match(listed_tests, executed_tests):
+    for shard_index, listed_shard in enumerate(listed_tests):
+        listed_shard_names = set(listed_shard)
+        executed_shard_names = set(executed_tests[shard_index])
+
+        listed_string = "\n".join(listed_shard_names)
+        exeucted_string = "\n".join(executed_shard_names)
+
+        assert listed_shard_names == executed_shard_names, (
+            "Executed tests do not match the listed tests:\nExecuted:\n{}\n\nListed:\n{}".format(exeucted_string, listed_string)
+        )
+
+def test_shards_cover_all_test(self_test_exe, test_case):
+    all_tests = list_tests(self_test_exe, test_case)
+    sharded_tests = [list_tests(self_test_exe, test_case, index) for index in range(test_case.shard_count)]
+
+    check_listed_tests_match(all_tests, sharded_tests)
+
+    executed_tests = [execute_tests(self_test_exe, test_case, index) for index in range(test_case.shard_count)]
+
+    check_listed_and_executed_tests_match(sharded_tests, executed_tests)
+
+
+def main():
+    self_test_exe, = sys.argv[1:]
+
+    # We want a random seed for the test, but want to avoid 0, because it has special meaning
+    shard_counts = [1, 5]
+    seeds = [random.randint(1, 2 ** 32 - 1), random.randint(1, 2 ** 32 - 1)]
+    tags = [["generators"], ["generators", "matchers"], []]
+    orders = ["rand", "decl", "lex"]
+
+    test_cases = [TestCase(*t) for t in itertools.product(shard_counts, orders, tags, seeds)]
+
+    # We use multiprocessing here because there are quite a few test cases, and running them
+    # serially is slow
+    pool = multiprocessing.Pool()
+    pool.starmap(test_shards_cover_all_test, itertools.product([self_test_exe], test_cases))
+
+if __name__ == '__main__':
+    sys.exit(main())
-- 
2.28.0.windows.1

